# Mamba-2 + 1a: sequential state space model with state_age embedding
# First Mamba-2 experiment. Uses 1a encoding (the Phase 1 winner).
# Compare directly against exp-1a-state-age-embed (MLP baseline).
#
# Architecture change:
#   MLP trunk (flatten + Linear(1960,512) + Linear(512,256))
#   → Mamba-2 (project(196,256) + 2× Mamba2Block + last timestep)
#
# Key differences from MLP:
#   - Temporal structure preserved (no flatten)
#   - Sequential SSM scan processes frames in order
#   - Controller conditioning via addition (not concatenation)
#   - ~850K params vs ~1.08M (fewer params, structured for time)

data:
  dataset_dir: null
  max_games: 2000
  stage_filter: null
  character_filter: null

encoding:
  xy_scale: 0.05
  percent_scale: 0.01
  shield_scale: 0.01
  action_vocab: 400
  action_embed_dim: 32
  jumps_vocab: 8
  jumps_embed_dim: 4
  # Exp 1a (Phase 1 winner)
  state_age_as_embed: true
  state_age_embed_vocab: 150
  state_age_embed_dim: 8
  press_events: false
  lookahead: 0

model:
  arch: mamba2
  context_len: 10
  d_model: 256
  d_state: 64
  n_layers: 2
  headdim: 64
  dropout: 0.1

training:
  lr: 0.001
  weight_decay: 0.00001
  batch_size: 512
  num_epochs: 2
  train_split: 0.9
  device: null

loss_weights:
  continuous: 1.0
  binary: 0.5
  action: 2.0
  jumps: 0.5

save_dir: worldmodel/checkpoints
