# Mamba-2 medium: first GPU validation run
# 4.3M params on 22K games. Proves full pipeline on GPU:
#   - All prediction heads (velocity, dynamics, combat context)
#   - ActionBreakdown category metrics
#   - Mamba-2 SSD scan on CUDA
#
# Expected: ~10 min/epoch on A100, ~2hr total for 10 epochs.
# If this works, scale to mamba2-large-gpu.yaml with 288K games.

data:
  dataset_dir: null
  max_games: 22000
  stage_filter: null
  character_filter: null

encoding:
  xy_scale: 0.05
  percent_scale: 0.01
  shield_scale: 0.01
  action_vocab: 400
  action_embed_dim: 32
  jumps_vocab: 8
  jumps_embed_dim: 4
  state_age_as_embed: true
  state_age_embed_vocab: 150
  state_age_embed_dim: 8
  press_events: false
  lookahead: 0

model:
  arch: mamba2
  context_len: 10
  d_model: 384
  d_state: 64
  n_layers: 4
  headdim: 64
  dropout: 0.1
  chunk_size: 10

training:
  lr: 0.0005
  weight_decay: 0.00001
  batch_size: 1024
  num_epochs: 10
  train_split: 0.9
  device: null
  log_interval: 1000  # ~60s between log lines on A100 (vs 30+ min default at 181K batches)
  scheduled_sampling: 0.3
  ss_noise_scale: 0.1
  ss_anneal_epochs: 3
  ss_corrupt_frames: 3

loss_weights:
  continuous: 1.0
  velocity: 0.5
  dynamics: 0.5
  binary: 0.5
  action: 2.0
  jumps: 0.5
  l_cancel: 0.3
  hurtbox: 0.3
  ground: 0.3
  last_attack: 0.3

save_dir: checkpoints/mamba2-medium-gpu
