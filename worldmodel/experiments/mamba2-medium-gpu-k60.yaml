# Mamba-2 medium K=60: same architecture as medium-gpu, 6x longer context
# Fair comparison against K=10 — identical model, encoding, loss weights.
# Only changes: context_len 10→60, chunk_size 10→30, batch_size 1024→512.
#
# K=60 = 1 second of Melee at 60fps. Covers neutral exchanges, combo
# sequences, and recovery situations. SSM should benefit from this.
# Batch_size halved because 6x longer sequences use more VRAM.

data:
  dataset_dir: null
  max_games: 22000
  stage_filter: null
  character_filter: null

encoding:
  xy_scale: 0.05
  percent_scale: 0.01
  shield_scale: 0.01
  action_vocab: 400
  action_embed_dim: 32
  jumps_vocab: 8
  jumps_embed_dim: 4
  state_age_as_embed: true
  state_age_embed_vocab: 150
  state_age_embed_dim: 8
  press_events: false
  lookahead: 0

model:
  arch: mamba2
  context_len: 60
  d_model: 384
  d_state: 64
  n_layers: 4
  headdim: 64
  dropout: 0.1
  chunk_size: 30

training:
  lr: 0.0005
  weight_decay: 0.00001
  batch_size: 512
  num_epochs: 10
  train_split: 0.9
  device: null

loss_weights:
  continuous: 1.0
  velocity: 0.5
  dynamics: 0.5
  binary: 0.5
  action: 2.0
  jumps: 0.5
  l_cancel: 0.3
  hurtbox: 0.3
  ground: 0.3
  last_attack: 0.3

save_dir: checkpoints/mamba2-medium-gpu-k60
